{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TẢI VÀ SETUP BỘ DỮ LIỆU small-PhoMT"
      ],
      "metadata": {
        "id": "uke6ylyBjkLb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZg6hsx-jc33",
        "outputId": "860944ac-d1ae-4166-d5fe-a299704b88f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder contents\n",
            "Processing file 1hoTd2hFwjSeFThlPm6YpN0NW5ePXS3Jc small-dev.json\n",
            "Processing file 1_3L25SH1_jaEfOjpmpgnfMik4N3MxSyn small-test.json\n",
            "Processing file 1-eG6FeF-v__rsf77iWurddahXbyjTYh5 small-train.json\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hoTd2hFwjSeFThlPm6YpN0NW5ePXS3Jc\n",
            "To: /content/PhoMT_data/small-dev.json\n",
            "100% 594k/594k [00:00<00:00, 32.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_3L25SH1_jaEfOjpmpgnfMik4N3MxSyn\n",
            "To: /content/PhoMT_data/small-test.json\n",
            "100% 669k/669k [00:00<00:00, 36.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-eG6FeF-v__rsf77iWurddahXbyjTYh5\n",
            "To: /content/PhoMT_data/small-train.json\n",
            "100% 5.68M/5.68M [00:00<00:00, 194MB/s]\n",
            "Download completed\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.9.0+cu126)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.15.2 torchmetrics-1.8.2\n"
          ]
        }
      ],
      "source": [
        "!gdown --folder https://drive.google.com/drive/folders/186OAOuSEYEDVcry7WP5UBdqECXo26QAb -O PhoMT_data\n",
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "import os\n",
        "from collections import Counter\n",
        "from torchmetrics.text.rouge import ROUGEScore\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Thiết lập device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uX7t9OvijuvZ",
        "outputId": "fa028403-13d6-47ae-cec4-8a3ec68271fc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XỬ LÝ DỮ LIỆU"
      ],
      "metadata": {
        "id": "1Bcz9SVXjyXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold=2):\n",
        "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "        self.freq_threshold = freq_threshold\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        frequencies = Counter()\n",
        "        idx = 4\n",
        "        for sentence in sentence_list:\n",
        "            for word in tokenize(sentence):\n",
        "                frequencies[word] += 1\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.stoi[word] = idx\n",
        "                    self.itos[idx] = word\n",
        "                    idx += 1\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        tokenized_text = tokenize(text)\n",
        "        return [self.stoi.get(token, self.stoi[\"<UNK>\"]) for token in tokenized_text]\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, json_file, src_vocab=None, tgt_vocab=None, is_train=True):\n",
        "        with open(json_file, 'r', encoding='utf-8') as f:\n",
        "            self.data = json.load(f)\n",
        "\n",
        "        # Sử dụng key chính xác: 'english' và 'vietnamese'\n",
        "        self.src_data = [item['english'] for item in self.data]\n",
        "        self.tgt_data = [item['vietnamese'] for item in self.data]\n",
        "\n",
        "        if is_train:\n",
        "            self.src_vocab = Vocabulary(freq_threshold=2)\n",
        "            self.src_vocab.build_vocabulary(self.src_data)\n",
        "            self.tgt_vocab = Vocabulary(freq_threshold=2)\n",
        "            self.tgt_vocab.build_vocabulary(self.tgt_data)\n",
        "        else:\n",
        "            self.src_vocab = src_vocab\n",
        "            self.tgt_vocab = tgt_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        src_text = self.src_data[index]\n",
        "        tgt_text = self.tgt_data[index]\n",
        "\n",
        "        src_indices = [self.src_vocab.stoi[\"<SOS>\"]] + self.src_vocab.numericalize(src_text) + [self.src_vocab.stoi[\"<EOS>\"]]\n",
        "        tgt_indices = [self.tgt_vocab.stoi[\"<SOS>\"]] + self.tgt_vocab.numericalize(tgt_text) + [self.tgt_vocab.stoi[\"<EOS>\"]]\n",
        "        return torch.tensor(src_indices), torch.tensor(tgt_indices)\n",
        "\n",
        "class Collate:\n",
        "    def __init__(self, pad_idx):\n",
        "        self.pad_idx = pad_idx\n",
        "    def __call__(self, batch):\n",
        "        src = [item[0] for item in batch]\n",
        "        tgt = [item[1] for item in batch]\n",
        "        src = nn.utils.rnn.pad_sequence(src, batch_first=True, padding_value=self.pad_idx) # batch_first=True cho dễ nhìn\n",
        "        tgt = nn.utils.rnn.pad_sequence(tgt, batch_first=True, padding_value=self.pad_idx)\n",
        "        return src, tgt\n",
        "\n",
        "# Setup Loaders\n",
        "train_path = 'PhoMT_data/small-train.json'\n",
        "test_path = 'PhoMT_data/small-test.json'\n",
        "\n",
        "train_dataset = TranslationDataset(train_path, is_train=True)\n",
        "test_dataset = TranslationDataset(test_path, src_vocab=train_dataset.src_vocab, tgt_vocab=train_dataset.tgt_vocab, is_train=False)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "pad_idx = train_dataset.src_vocab.stoi[\"<PAD>\"]\n",
        "loader_train = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=Collate(pad_idx))\n",
        "loader_test = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=Collate(pad_idx))\n",
        "\n",
        "print(\"Dữ liệu đã sẵn sàng!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UcPXWZJj2Zi",
        "outputId": "cbacd45d-af1f-409d-f1c3-47f09d8d52d1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dữ liệu đã sẵn sàng!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BÀI 1**"
      ],
      "metadata": {
        "id": "iVmx2io8mKnT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XÂY DỰNG MÔ HÌNH ENCODER - DECODER 3 LỚP LSTM"
      ],
      "metadata": {
        "id": "w30IpCoikZZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Định nghĩa Model\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src: [batch, seq_len]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        return hidden, cell\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        # input: [batch]\n",
        "        input = input.unsqueeze(1) # [batch, 1]\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        prediction = self.fc_out(output.squeeze(1))\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.shape[0]\n",
        "        tgt_len = tgt.shape[1]\n",
        "        tgt_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        input = tgt[:, 0] # <SOS>\n",
        "\n",
        "        for t in range(1, tgt_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            outputs[:, t, :] = output\n",
        "            top1 = output.argmax(1)\n",
        "            input = tgt[:, t] if random.random() < teacher_forcing_ratio else top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "# Khởi tạo Model\n",
        "INPUT_DIM = len(train_dataset.src_vocab)\n",
        "OUTPUT_DIM = len(train_dataset.tgt_vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HIDDEN_DIM = 256\n",
        "N_LAYERS = 3\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HIDDEN_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HIDDEN_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ],
      "metadata": {
        "id": "DRD53tCXkiPo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HUẤN LUYỆN MÔ HÌNH"
      ],
      "metadata": {
        "id": "yWOm-uYUklbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    progress_bar = tqdm(iterator, desc=\"Training\", unit=\"batch\")\n",
        "\n",
        "    for i, (src, tgt) in enumerate(progress_bar):\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt)\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[:, 1:, :].reshape(-1, output_dim)\n",
        "        tgt = tgt[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, tgt)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "N_EPOCHS = 5\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train_fn(model, loader_train, optimizer, criterion, 1)\n",
        "    end_time = time.time()\n",
        "    mins = int((end_time - start_time) / 60)\n",
        "    secs = int((end_time - start_time) % 60)\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {mins}m {secs}s | Train Loss: {train_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvPjz44Fklqh",
        "outputId": "008f39f3-d24a-4a12-86d7-0ede3bb6dd71"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== BẮT ĐẦU TRAIN BÀI 1 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [01:34<00:00,  3.32batch/s, loss=6.14]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 1m 34s | Train Loss: 6.203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [01:30<00:00,  3.47batch/s, loss=5.99]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 02 | Time: 1m 30s | Train Loss: 5.976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [01:33<00:00,  3.34batch/s, loss=5.82]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 03 | Time: 1m 33s | Train Loss: 5.857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [01:32<00:00,  3.38batch/s, loss=5.81]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 04 | Time: 1m 32s | Train Loss: 5.764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [01:32<00:00,  3.38batch/s, loss=5.72]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 05 | Time: 1m 32s | Train Loss: 5.665\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ĐÁNH GIÁ MÔ HÌNH"
      ],
      "metadata": {
        "id": "-SY600Iokq0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(model, sentence, src_vocab, tgt_vocab, device, max_length=50):\n",
        "    model.eval()\n",
        "\n",
        "    if isinstance(sentence, str):\n",
        "        tokens = tokenize(sentence)\n",
        "    else:\n",
        "        tokens = sentence\n",
        "\n",
        "    text_to_indices = [src_vocab.stoi[\"<SOS>\"]] + [src_vocab.stoi.get(token, src_vocab.stoi[\"<UNK>\"]) for token in tokens] + [src_vocab.stoi[\"<EOS>\"]]\n",
        "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(sentence_tensor)\n",
        "\n",
        "    outputs = [tgt_vocab.stoi[\"<SOS>\"]]\n",
        "    for _ in range(max_length):\n",
        "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n",
        "            best_guess = output.argmax(1).item()\n",
        "\n",
        "        if best_guess == tgt_vocab.stoi[\"<EOS>\"]:\n",
        "            break\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "    # Chuyển indices ngược lại thành từ\n",
        "    translated_sentence = [tgt_vocab.itos[idx] for idx in outputs]\n",
        "    return translated_sentence[1:] # Bỏ <SOS>\n",
        "\n",
        "# Đánh giá ROUGE\n",
        "def evaluate_rouge(model, iterator, src_vocab, tgt_vocab):\n",
        "    model.eval()\n",
        "    rouge = ROUGEScore()\n",
        "    preds, targets = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in tqdm(iterator, desc=\"Evaluating\"):\n",
        "            src = src.to(device)\n",
        "            hidden, cell = model.encoder(src)\n",
        "            batch_preds = []\n",
        "\n",
        "            # Greedy Decode\n",
        "            input = torch.tensor([tgt_vocab.stoi[\"<SOS>\"]] * src.shape[0]).to(device)\n",
        "            for _ in range(50): # Max length\n",
        "                output, hidden, cell = model.decoder(input, hidden, cell)\n",
        "                input = output.argmax(1)\n",
        "                batch_preds.append(input.unsqueeze(1))\n",
        "\n",
        "            batch_preds = torch.cat(batch_preds, dim=1) # [batch, len]\n",
        "\n",
        "            # Convert to string\n",
        "            for i in range(src.shape[0]):\n",
        "                pred_tokens = [tgt_vocab.itos[idx.item()] for idx in batch_preds[i]]\n",
        "                target_tokens = [tgt_vocab.itos[idx.item()] for idx in tgt[i] if idx.item() not in [0,1,2]] # Bỏ pad, sos, eos\n",
        "\n",
        "                # Cắt ở EOS đầu tiên cho pred\n",
        "                if \"<EOS>\" in pred_tokens:\n",
        "                    pred_tokens = pred_tokens[:pred_tokens.index(\"<EOS>\")]\n",
        "\n",
        "                preds.append(\" \".join(pred_tokens))\n",
        "                targets.append(\" \".join(target_tokens))\n",
        "\n",
        "    scores = rouge(preds, targets)\n",
        "    return scores\n",
        "\n",
        "rouge_scores = evaluate_rouge(model, loader_test, train_dataset.src_vocab, train_dataset.tgt_vocab)\n",
        "print(f\"ROUGE-L Fmeasure: {rouge_scores['rougeL_fmeasure'].item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwYheZZakvNx",
        "outputId": "cf79bf38-96f1-4ac8-943a-977892914ab9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Đang tính ROUGE cho Bài 1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 10.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE-L Fmeasure: 0.1880\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BÀI 2**"
      ],
      "metadata": {
        "id": "IHBxNjCokuAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XÂY DỰNG MÔ HÌNH ENCODER (3 LSTM) - DECODER (3LSTM), ATTENTION Bahdanau"
      ],
      "metadata": {
        "id": "8aOG7OqvnWsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderAttention(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        # batch_first=True\n",
        "        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src: [batch, seq_len]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "\n",
        "        return outputs, hidden, cell\n",
        "\n",
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.W_q = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
        "        self.W_k = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
        "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, decoder_hidden, encoder_outputs):\n",
        "        # decoder_hidden: [batch, hidden] (top layer)\n",
        "        # encoder_outputs: [batch, src_len, hidden]\n",
        "        query = self.W_q(decoder_hidden).unsqueeze(1)\n",
        "        keys = self.W_k(encoder_outputs)\n",
        "        energy = torch.tanh(query + keys)\n",
        "        scores = self.v(energy).squeeze(2)\n",
        "        attention = torch.softmax(scores, dim=1)\n",
        "        context = torch.bmm(attention.unsqueeze(1), encoder_outputs)\n",
        "        return context\n",
        "\n",
        "class DecoderAttention(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers, dropout, attention):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
        "        self.fc_out = nn.Linear(hidden_dim * 2, output_dim) # Concat output + context\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell, encoder_outputs):\n",
        "        input = input.unsqueeze(1)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "\n",
        "        # Dùng hidden state lớp cuối cùng của decoder để tính attention\n",
        "        context = self.attention(hidden[-1], encoder_outputs)\n",
        "\n",
        "        combined = torch.cat((output, context), dim=2)\n",
        "        prediction = self.fc_out(combined.squeeze(1))\n",
        "\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "class Seq2SeqAttention(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.shape[0]\n",
        "        tgt_len = tgt.shape[1]\n",
        "        tgt_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
        "        encoder_outputs, hidden, cell = self.encoder(src)\n",
        "\n",
        "        input = tgt[:, 0]\n",
        "\n",
        "        for t in range(1, tgt_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n",
        "            outputs[:, t, :] = output\n",
        "            top1 = output.argmax(1)\n",
        "            input = tgt[:, t] if random.random() < teacher_forcing_ratio else top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "# Hyperparameters\n",
        "INPUT_DIM = len(train_dataset.src_vocab)\n",
        "OUTPUT_DIM = len(train_dataset.tgt_vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HIDDEN_DIM = 256\n",
        "N_LAYERS = 3\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "attn = BahdanauAttention(HIDDEN_DIM)\n",
        "enc = EncoderAttention(INPUT_DIM, ENC_EMB_DIM, HIDDEN_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = DecoderAttention(OUTPUT_DIM, DEC_EMB_DIM, HIDDEN_DIM, N_LAYERS, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2SeqAttention(enc, dec, device).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ],
      "metadata": {
        "id": "9pDkE3twnbw4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HUẤN LUYỆN MÔ HÌNH"
      ],
      "metadata": {
        "id": "YqL7N2u5n24E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn_attn(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    progress_bar = tqdm(iterator, desc=\"Training Attention\", unit=\"batch\")\n",
        "\n",
        "    for i, (src, tgt) in enumerate(progress_bar):\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt)\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[:, 1:, :].reshape(-1, output_dim)\n",
        "        tgt = tgt[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, tgt)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "N_EPOCHS = 5\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train_fn_attn(model, loader_train, optimizer, criterion, 1)\n",
        "    end_time = time.time()\n",
        "    mins = int((end_time - start_time) / 60)\n",
        "    secs = int((end_time - start_time) % 60)\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {mins}m {secs}s | Train Loss: {train_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-zwCXOEn3JI",
        "outputId": "bd407f29-e5a9-44eb-fea6-770989a1d9a3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Attention: 100%|██████████| 313/313 [02:06<00:00,  2.48batch/s, loss=5.91]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 2m 6s | Train Loss: 6.136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Attention: 100%|██████████| 313/313 [02:09<00:00,  2.41batch/s, loss=5.82]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 02 | Time: 2m 9s | Train Loss: 5.869\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Attention: 100%|██████████| 313/313 [02:13<00:00,  2.34batch/s, loss=5.35]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 03 | Time: 2m 13s | Train Loss: 5.621\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Attention: 100%|██████████| 313/313 [02:08<00:00,  2.43batch/s, loss=5.51]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 04 | Time: 2m 8s | Train Loss: 5.410\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Attention: 100%|██████████| 313/313 [02:09<00:00,  2.43batch/s, loss=5.22]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 05 | Time: 2m 9s | Train Loss: 5.239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ĐÁNH GIÁ MÔ HÌNH"
      ],
      "metadata": {
        "id": "Yp-wZKtEoMzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_rouge_attn(model, iterator, src_vocab, tgt_vocab):\n",
        "    model.eval()\n",
        "    rouge = ROUGEScore()\n",
        "    preds, targets = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in tqdm(iterator, desc=\"Evaluating\"):\n",
        "            src = src.to(device)\n",
        "            # Encoder trả về thêm encoder_outputs\n",
        "            encoder_outputs, hidden, cell = model.encoder(src)\n",
        "            batch_preds = []\n",
        "\n",
        "            input = torch.tensor([tgt_vocab.stoi[\"<SOS>\"]] * src.shape[0]).to(device)\n",
        "            for _ in range(50):\n",
        "                # Decoder nhận thêm encoder_outputs\n",
        "                output, hidden, cell = model.decoder(input, hidden, cell, encoder_outputs)\n",
        "                input = output.argmax(1)\n",
        "                batch_preds.append(input.unsqueeze(1))\n",
        "\n",
        "            batch_preds = torch.cat(batch_preds, dim=1)\n",
        "\n",
        "            for i in range(src.shape[0]):\n",
        "                pred_tokens = [tgt_vocab.itos[idx.item()] for idx in batch_preds[i]]\n",
        "                target_tokens = [tgt_vocab.itos[idx.item()] for idx in tgt[i] if idx.item() not in [0,1,2]]\n",
        "\n",
        "                if \"<EOS>\" in pred_tokens:\n",
        "                    pred_tokens = pred_tokens[:pred_tokens.index(\"<EOS>\")]\n",
        "\n",
        "                preds.append(\" \".join(pred_tokens))\n",
        "                targets.append(\" \".join(target_tokens))\n",
        "\n",
        "    scores = rouge(preds, targets)\n",
        "    return scores\n",
        "\n",
        "rouge_scores = evaluate_rouge_attn(model, loader_test, train_dataset.src_vocab, train_dataset.tgt_vocab)\n",
        "print(f\"ROUGE-L Fmeasure: {rouge_scores['rougeL_fmeasure'].item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wydW9QMqoQk9",
        "outputId": "2bc14190-3479-42b9-afec-7ebecf24f9a9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 11.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE-L Fmeasure: 0.2922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BÀI 3**"
      ],
      "metadata": {
        "id": "UEkKj9bsnPIA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XÂY DỰNG MÔ HÌNH ENCODER - DECODER, ATTENTION Luong"
      ],
      "metadata": {
        "id": "oN0K-YgnvIyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src: [batch, src len]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "\n",
        "        # outputs: [batch, src len, hidden_dim]\n",
        "        # hidden, cell: [n_layers, batch, hidden_dim]\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "\n",
        "        return outputs, hidden, cell\n",
        "\n",
        "class LuongAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        # Luong Attention \"General\": score(h_t, h_s) = h_t^T * W * h_s\n",
        "        self.W = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
        "\n",
        "    def forward(self, decoder_hidden, encoder_outputs):\n",
        "        # decoder_hidden: [batch, hidden_dim] (Trạng thái hiện tại của decoder)\n",
        "        # encoder_outputs: [batch, src_len, hidden_dim]\n",
        "\n",
        "        # query: [batch, 1, hidden_dim]\n",
        "        query = self.W(decoder_hidden).unsqueeze(1)\n",
        "\n",
        "        # Alignment Scores\n",
        "        # [batch, 1, hidden] * [batch, src_len, hidden] (transpose) -> [batch, 1, src_len]\n",
        "        scores = torch.bmm(query, encoder_outputs.transpose(1, 2))\n",
        "\n",
        "        attention_weights = torch.softmax(scores, dim=-1) # [batch, 1, src_len]\n",
        "\n",
        "        # Context Vector (Weighted Sum)\n",
        "        # [batch, 1, src_len] * [batch, src_len, hidden] -> [batch, 1, hidden]\n",
        "        context = torch.bmm(attention_weights, encoder_outputs)\n",
        "\n",
        "        return context, attention_weights\n",
        "\n",
        "class DecoderLuong(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers, dropout, attention):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
        "\n",
        "        # Layer kết hợp Context và Hidden state (Concat Layer)\n",
        "        # Input: [Hidden Decoder; Context Vector] -> Output: Hidden Size\n",
        "        self.wc = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "\n",
        "        # Layer dự đoán từ cuối cùng\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, input, hidden, cell, encoder_outputs):\n",
        "        # input: [batch]\n",
        "        # hidden, cell: [n_layers, batch, hidden]\n",
        "        # encoder_outputs: [batch, src_len, hidden]\n",
        "\n",
        "        input = input.unsqueeze(1)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "        # rnn_output: [batch, 1, hidden]\n",
        "        rnn_output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "\n",
        "        # rnn_output đang có dạng [batch, 1, hidden] -> squeeze -> [batch, hidden]\n",
        "        context, attn_weights = self.attention(rnn_output.squeeze(1), encoder_outputs)\n",
        "        # context: [batch, 1, hidden]\n",
        "\n",
        "        # Luong Attention\n",
        "        # cat: [batch, 1, hidden * 2]\n",
        "        combined = torch.cat((rnn_output, context), dim=2)\n",
        "\n",
        "        # Chiếu về hidden dim và qua Tanh\n",
        "        h_tilde = torch.tanh(self.wc(combined)) # [batch, 1, hidden]\n",
        "\n",
        "        prediction = self.fc_out(h_tilde.squeeze(1)) # [batch, output_dim]\n",
        "\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "class Seq2SeqLuong(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
        "        # src: [batch, src len]\n",
        "        # tgt: [batch, tgt len]\n",
        "\n",
        "        batch_size = src.shape[0]\n",
        "        tgt_len = tgt.shape[1]\n",
        "        tgt_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
        "\n",
        "        # Encoder\n",
        "        encoder_outputs, hidden, cell = self.encoder(src)\n",
        "\n",
        "        # Decoder Input đầu tiên (<SOS>)\n",
        "        input = tgt[:, 0]\n",
        "\n",
        "        for t in range(1, tgt_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n",
        "\n",
        "            outputs[:, t, :] = output\n",
        "\n",
        "            # Teacher Forcing\n",
        "            top1 = output.argmax(1)\n",
        "            input = tgt[:, t] if random.random() < teacher_forcing_ratio else top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "# Hyperparameters\n",
        "INPUT_DIM = len(train_dataset.src_vocab)\n",
        "OUTPUT_DIM = len(train_dataset.tgt_vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HIDDEN_DIM = 256\n",
        "N_LAYERS = 3\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "attn = LuongAttention(HIDDEN_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HIDDEN_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = DecoderLuong(OUTPUT_DIM, DEC_EMB_DIM, HIDDEN_DIM, N_LAYERS, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2SeqLuong(enc, dec, device).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "pad_idx = train_dataset.tgt_vocab.stoi[\"<PAD>\"]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "\n",
        "print(f'Mô hình Luong Attention có {sum(p.numel() for p in model.parameters() if p.requires_grad):,} tham số.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08BaiFElutjw",
        "outputId": "f46ececb-f12b-4226-ffae-5b7af434c883"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mô hình Luong Attention có 8,148,440 tham số.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HUẤN LUYỆN MÔ HÌNH"
      ],
      "metadata": {
        "id": "733pRR0tvVJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    progress_bar = tqdm(iterator, desc=\"Training Luong\", unit=\"batch\")\n",
        "\n",
        "    for i, (src, tgt) in enumerate(progress_bar):\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt)\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[:, 1:, :].reshape(-1, output_dim)\n",
        "        tgt = tgt[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, tgt)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train_fn(model, loader_train, optimizer, criterion, CLIP)\n",
        "    end_time = time.time()\n",
        "    mins = int((end_time - start_time) / 60)\n",
        "    secs = int((end_time - start_time) % 60)\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {mins}m {secs}s | Train Loss: {train_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9lTayG6uyii",
        "outputId": "11dfd3cf-b884-4c74-99cd-4b7b770cf553"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Luong: 100%|██████████| 313/313 [01:51<00:00,  2.80batch/s, loss=5.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 1m 51s | Train Loss: 5.532\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Luong: 100%|██████████| 313/313 [01:50<00:00,  2.84batch/s, loss=5.35]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 02 | Time: 1m 50s | Train Loss: 5.390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Luong: 100%|██████████| 313/313 [01:51<00:00,  2.80batch/s, loss=5.26]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 03 | Time: 1m 51s | Train Loss: 5.291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Luong: 100%|██████████| 313/313 [01:51<00:00,  2.82batch/s, loss=4.94]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 04 | Time: 1m 51s | Train Loss: 5.189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Luong: 100%|██████████| 313/313 [01:51<00:00,  2.81batch/s, loss=5.06]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 05 | Time: 1m 51s | Train Loss: 5.100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Luong: 100%|██████████| 313/313 [01:51<00:00,  2.81batch/s, loss=5.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 06 | Time: 1m 51s | Train Loss: 5.018\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Luong: 100%|██████████| 313/313 [01:51<00:00,  2.82batch/s, loss=4.78]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 07 | Time: 1m 51s | Train Loss: 4.938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Luong: 100%|██████████| 313/313 [01:52<00:00,  2.78batch/s, loss=4.89]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 08 | Time: 1m 52s | Train Loss: 4.869\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Luong: 100%|██████████| 313/313 [01:54<00:00,  2.74batch/s, loss=4.81]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 09 | Time: 1m 54s | Train Loss: 4.807\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Luong: 100%|██████████| 313/313 [01:52<00:00,  2.79batch/s, loss=4.68]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10 | Time: 1m 52s | Train Loss: 4.742\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ĐÁNH GIÁ MÔ HÌNH"
      ],
      "metadata": {
        "id": "-jF8Czhbvdv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_rouge(model, iterator, src_vocab, tgt_vocab):\n",
        "    model.eval()\n",
        "    rouge = ROUGEScore()\n",
        "    preds, targets = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in tqdm(iterator, desc=\"Evaluating\"):\n",
        "            src = src.to(device)\n",
        "            encoder_outputs, hidden, cell = model.encoder(src)\n",
        "            batch_preds = []\n",
        "\n",
        "            input = torch.tensor([tgt_vocab.stoi[\"<SOS>\"]] * src.shape[0]).to(device)\n",
        "\n",
        "            for _ in range(50):\n",
        "                output, hidden, cell = model.decoder(input, hidden, cell, encoder_outputs)\n",
        "                input = output.argmax(1)\n",
        "                batch_preds.append(input.unsqueeze(1))\n",
        "\n",
        "            batch_preds = torch.cat(batch_preds, dim=1)\n",
        "\n",
        "            for i in range(src.shape[0]):\n",
        "                pred_tokens = [tgt_vocab.itos[idx.item()] for idx in batch_preds[i]]\n",
        "                if \"<EOS>\" in pred_tokens:\n",
        "                    pred_tokens = pred_tokens[:pred_tokens.index(\"<EOS>\")]\n",
        "\n",
        "                target_tokens = [tgt_vocab.itos[idx.item()] for idx in tgt[i] if idx.item() not in [0, 1, 2]]\n",
        "\n",
        "                preds.append(\" \".join(pred_tokens))\n",
        "                targets.append(\" \".join(target_tokens))\n",
        "\n",
        "    scores = rouge(preds, targets)\n",
        "    return scores\n",
        "\n",
        "rouge_scores = evaluate_rouge(model, loader_test, train_dataset.src_vocab, train_dataset.tgt_vocab)\n",
        "print(f\"ROUGE-L Fmeasure: {rouge_scores['rougeL_fmeasure'].item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7kXWi19vhJ3",
        "outputId": "ede56d0e-d571-4eca-a78b-262259f4176e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 11.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE-L Fmeasure: 0.3431\n"
          ]
        }
      ]
    }
  ]
}