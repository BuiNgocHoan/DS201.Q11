{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BÀI THỰC HÀNH 5: KIẾN TRÚC TRANSFORMER ENCODER"
      ],
      "metadata": {
        "id": "52gZkXCI1iiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvi --quiet\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pyvi import ViTokenizer\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Thiết lập thiết bị (GPU nếu có)\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Đang chạy trên: {DEVICE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zp-BDND9wSnX",
        "outputId": "399b829e-c176-4371-f818-1d59ecf6e34f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Đang chạy trên: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bài 1: Xây dựng mô hình Transformer Encoder gồm 3 lớp theo mô tả trong nghiên cứu [Attention is all you need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf). Huấn luyện mô hình này cho bài toán phân loại domain câu bình luận trên bộ dữ liệu [UIT-ViOCD](https://drive.google.com/drive/folders/1Lu9axyLkw7dMx80uLRgvCnZsmNzhJWAa?usp=sharing)."
      ],
      "metadata": {
        "id": "vwZXS8WC1gP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 32\n",
        "VOCAB_SIZE = 20000\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_df = pd.read_json('train.json', orient='index')\n",
        "dev_df = pd.read_json('dev.json', orient='index')\n",
        "test_df = pd.read_json('test.json', orient='index')\n",
        "\n",
        "TEXT_COL = 'review'\n",
        "LABEL_COL = 'domain'\n",
        "\n",
        "print(f\"Số dòng Train: {len(train_df)} | Mẫu: {train_df[TEXT_COL].iloc[0][:50]}...\")\n",
        "\n",
        "def tokenizer(text):\n",
        "    return ViTokenizer.tokenize(str(text).lower()).split()\n",
        "\n",
        "all_train_words = []\n",
        "for text in train_df[TEXT_COL]:\n",
        "    all_train_words.extend(tokenizer(text))\n",
        "\n",
        "word_counts = Counter(all_train_words)\n",
        "vocab_list = ['<PAD>', '<UNK>'] + [w for w, c in word_counts.most_common(VOCAB_SIZE)]\n",
        "vocab_to_int = {w: i for i, w in enumerate(vocab_list)}\n",
        "PAD_IDX, UNK_IDX = 0, 1\n",
        "REAL_VOCAB_SIZE = len(vocab_to_int)\n",
        "\n",
        "le = LabelEncoder()\n",
        "le.fit(pd.concat([train_df[LABEL_COL], dev_df[LABEL_COL], test_df[LABEL_COL]]))\n",
        "NUM_CLASSES = len(le.classes_)\n",
        "\n",
        "print(f\"Vocab={REAL_VOCAB_SIZE} | Số nhãn={NUM_CLASSES}\")\n",
        "\n",
        "class ViOCDDataset(Dataset):\n",
        "    def __init__(self, df, vocab, max_len):\n",
        "        self.texts = df[TEXT_COL].values\n",
        "        self.labels = le.transform(df[LABEL_COL].values)\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_len\n",
        "    def __len__(self): return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = tokenizer(self.texts[idx])\n",
        "        encoded = [self.vocab.get(w, UNK_IDX) for w in tokens]\n",
        "        if len(encoded) < self.max_len: encoded += [PAD_IDX] * (self.max_len - len(encoded))\n",
        "        else: encoded = encoded[:self.max_len]\n",
        "        return torch.tensor(encoded, dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "\n",
        "train_loader = DataLoader(ViOCDDataset(train_df, vocab_to_int, MAX_LEN), batch_size=BATCH_SIZE, shuffle=True)\n",
        "dev_loader = DataLoader(ViOCDDataset(dev_df, vocab_to_int, MAX_LEN), batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(ViOCDDataset(test_df, vocab_to_int, MAX_LEN), batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3M0H5ljwVyN",
        "outputId": "198daadd-339a-45aa-e6a6-d4f669802327"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Số dòng Train: 4387 | Mẫu: gói hàng cẩn thận . chơi pubg với liên q...\n",
            "Vocab=6358 | Số nhãn=4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Số dòng Dev: {len(dev_df)} | Mẫu: {dev_df[TEXT_COL].iloc[0][:50]}...\")\n",
        "print(f\"Số dòng Test: {len(test_df)} | Mẫu: {test_df[TEXT_COL].iloc[0][:50]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSmV0S3Tz_xo",
        "outputId": "5936907b-c296-479a-f32c-5b160a615951"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Số dòng Dev: 548 | Mẫu: quần đẹp nhưng cỡ lồn hơi ngắn...\n",
            "Số dòng Test: 549 | Mẫu: dẹp xinh giá rẻ , đáng mua...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "    def forward(self, x): return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, ff_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, ff_dim), nn.ReLU(),\n",
        "            nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_out, _ = self.attention(x, x, x, key_padding_mask=mask)\n",
        "        x = self.norm1(x + self.dropout(attn_out))\n",
        "        x = self.norm2(x + self.dropout(self.ffn(x)))\n",
        "        return x\n",
        "\n",
        "class ViOCDTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, ff_dim, num_layers, num_classes, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=PAD_IDX)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, MAX_LEN)\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(d_model, num_heads, ff_dim, dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mask = (x == PAD_IDX)\n",
        "        x = self.dropout(self.embedding(x) * math.sqrt(x.size(-1)))\n",
        "        x = self.pos_encoding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask=mask)\n",
        "        return self.fc(x.mean(dim=1))\n",
        "\n",
        "# Khởi tạo mô hình 3 lớp Encoder\n",
        "model = ViOCDTransformer(\n",
        "    vocab_size=REAL_VOCAB_SIZE,\n",
        "    d_model=128, num_heads=8, ff_dim=512,\n",
        "    num_layers=3, num_classes=NUM_CLASSES\n",
        ").to(DEVICE)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "abUWwhlCwXeL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\">>> Bắt đầu huấn luyện...\")\n",
        "for epoch in range(15):\n",
        "    model.train()\n",
        "    t_loss, t_correct = 0, 0\n",
        "    for texts, labels in train_loader:\n",
        "        texts, labels = texts.to(DEVICE), labels.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(texts)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        t_loss += loss.item()\n",
        "        t_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "    model.eval()\n",
        "    v_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in dev_loader:\n",
        "            texts, labels = texts.to(DEVICE), labels.to(DEVICE)\n",
        "            v_correct += (model(texts).argmax(1) == labels).sum().item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02} | Loss: {t_loss/len(train_loader):.4f} | Train Acc: {t_correct/len(train_df)*100:6.2f}% | Val Acc: {v_correct/len(dev_df)*100:6.2f}%\")\n",
        "\n",
        "model.eval()\n",
        "test_correct = 0\n",
        "with torch.no_grad():\n",
        "    for texts, labels in test_loader:\n",
        "        texts, labels = texts.to(DEVICE), labels.to(DEVICE)\n",
        "        test_correct += (model(texts).argmax(1) == labels).sum().item()\n",
        "\n",
        "print(f\"\\nĐỘ CHÍNH XÁC TRÊN TẬP TEST: {test_correct/len(test_df)*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VI14CcnDzMt1",
        "outputId": "7a355b4c-f1a0-4595-b1d9-52cd0aba37b2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Bắt đầu huấn luyện...\n",
            "Epoch 01 | Loss: 1.0571 | Train Acc:  58.49% | Val Acc:  72.26%\n",
            "Epoch 02 | Loss: 0.7341 | Train Acc:  72.65% | Val Acc:  77.92%\n",
            "Epoch 03 | Loss: 0.6144 | Train Acc:  77.05% | Val Acc:  81.20%\n",
            "Epoch 04 | Loss: 0.5449 | Train Acc:  79.62% | Val Acc:  83.58%\n",
            "Epoch 05 | Loss: 0.4828 | Train Acc:  82.43% | Val Acc:  84.12%\n",
            "Epoch 06 | Loss: 0.4467 | Train Acc:  84.02% | Val Acc:  85.40%\n",
            "Epoch 07 | Loss: 0.4595 | Train Acc:  83.38% | Val Acc:  85.77%\n",
            "Epoch 08 | Loss: 0.4007 | Train Acc:  85.46% | Val Acc:  87.41%\n",
            "Epoch 09 | Loss: 0.3800 | Train Acc:  85.96% | Val Acc:  86.50%\n",
            "Epoch 10 | Loss: 0.3613 | Train Acc:  86.98% | Val Acc:  87.59%\n",
            "Epoch 11 | Loss: 0.3556 | Train Acc:  87.14% | Val Acc:  87.23%\n",
            "Epoch 12 | Loss: 0.3464 | Train Acc:  87.39% | Val Acc:  87.96%\n",
            "Epoch 13 | Loss: 0.3168 | Train Acc:  88.65% | Val Acc:  87.96%\n",
            "Epoch 14 | Loss: 0.3142 | Train Acc:  88.74% | Val Acc:  86.86%\n",
            "Epoch 15 | Loss: 0.3079 | Train Acc:  89.38% | Val Acc:  86.68%\n",
            "\n",
            "ĐỘ CHÍNH XÁC TRÊN TẬP TEST: 90.35%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bài 2: Xây dựng mô hình Transformer Encoder gồm 3 lớp theo mô tả trong nghiên cứu [Attention is all you need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf). Huấn luyện mô hình này cho bài toán gán nhãn chuỗi trên bộ dữ liệu [PhoNERT](https://github.com/VinAIResearch/PhoNER_COVID19)."
      ],
      "metadata": {
        "id": "x7eBSgZH1mwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvi --quiet\n",
        "import os\n",
        "import json\n",
        "\n",
        "if not os.path.exists('PhoNER_COVID19'):\n",
        "    !git clone https://github.com/VinAIResearch/PhoNER_COVID19.git\n",
        "\n",
        "def load_phonert_json_correct(split):\n",
        "    path = f'PhoNER_COVID19/data/syllable/{split}_syllable.json'\n",
        "    sentences = []\n",
        "    tags = []\n",
        "\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                item = json.loads(line)\n",
        "                sentences.append(item['words'])\n",
        "                tags.append(item['tags'])\n",
        "    return sentences, tags\n",
        "\n",
        "train_sents, train_tags = load_phonert_json_correct('train')\n",
        "dev_sents, dev_tags = load_phonert_json_correct('dev')\n",
        "test_sents, test_tags = load_phonert_json_correct('test')\n",
        "\n",
        "print(f\"Train: {len(train_sents)} câu.\")\n",
        "print(f\"Ví dụ: {train_sents[0][:5]}... -> {train_tags[0][:5]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frhvRqYI1mHx",
        "outputId": "ff5cffe1-a944-45c0-b060-600963e18b84"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 5027 câu.\n",
            "Ví dụ: ['Đồng', 'thời', ',', 'bệnh', 'viện']... -> ['O', 'O', 'O', 'O', 'O']...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Xây dựng Vocab\n",
        "all_words = [w for s in train_sents for w in s]\n",
        "word_counts = Counter(all_words)\n",
        "vocab = ['<PAD>', '<UNK>'] + [w for w, c in word_counts.most_common(20000)]\n",
        "word2idx = {w: i for i, w in enumerate(vocab)}\n",
        "\n",
        "# Lấy tất cả các nhãn duy nhất từ tập Train\n",
        "all_tags_list = sorted(list(set([t for s in train_tags for t in s])))\n",
        "tag2idx = {t: i for i, t in enumerate(all_tags_list)}\n",
        "idx2tag = {i: t for t, i in tag2idx.items()}\n",
        "NUM_TAGS = len(tag2idx)\n",
        "\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 32\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class PhoNERDataset(Dataset):\n",
        "    def __init__(self, sents, tags, word2idx, tag2idx, max_len):\n",
        "        self.sents = sents\n",
        "        self.tags = tags\n",
        "        self.word2idx = word2idx\n",
        "        self.tag2idx = tag2idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self): return len(self.sents)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        w_ids = [self.word2idx.get(w, 1) for w in self.sents[idx]] # 1 là <UNK>\n",
        "        t_ids = [self.tag2idx[t] for t in self.tags[idx]]\n",
        "\n",
        "        # Padding\n",
        "        if len(w_ids) < self.max_len:\n",
        "            w_ids += [0] * (self.max_len - len(w_ids))\n",
        "            t_ids += [-100] * (self.max_len - len(t_ids))\n",
        "        else:\n",
        "            w_ids = w_ids[:self.max_len]\n",
        "            t_ids = t_ids[:self.max_len]\n",
        "\n",
        "        return torch.tensor(w_ids), torch.tensor(t_ids)\n",
        "\n",
        "train_loader = DataLoader(PhoNERDataset(train_sents, train_tags, word2idx, tag2idx, MAX_LEN), batch_size=BATCH_SIZE, shuffle=True)\n",
        "dev_loader = DataLoader(PhoNERDataset(dev_sents, dev_tags, word2idx, tag2idx, MAX_LEN), batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(PhoNERDataset(test_sents, test_tags, word2idx, tag2idx, MAX_LEN), batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "j8mlKvPx1_4_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "    def forward(self, x): return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_ff), nn.ReLU(),\n",
        "            nn.Dropout(dropout), nn.Linear(dim_ff, d_model)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_out, _ = self.attn(x, x, x, key_padding_mask=mask)\n",
        "        x = self.norm1(x + self.dropout(attn_out))\n",
        "        x = self.norm2(x + self.dropout(self.ffn(x)))\n",
        "        return x\n",
        "\n",
        "class TransformerNER(nn.Module):\n",
        "    def __init__(self, vocab_size, num_tags, d_model=128, nhead=8, num_layers=3, dim_ff=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, MAX_LEN)\n",
        "\n",
        "        # Khối 3 lớp Encoder\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            TransformerBlock(d_model, nhead, dim_ff, dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.fc = nn.Linear(d_model, num_tags)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mask = (x == 0)\n",
        "        x = self.dropout(self.embedding(x) * math.sqrt(x.size(-1)))\n",
        "        x = self.pos_encoding(x)\n",
        "\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x, mask=mask)\n",
        "\n",
        "        return self.fc(x) # Output: [Batch, SeqLen, NumTags]\n",
        "\n",
        "model = TransformerNER(len(vocab), NUM_TAGS).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "gtkRv8Fn3kW-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ner(model, loader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for w, t in loader:\n",
        "        w, t = w.to(DEVICE), t.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(w)\n",
        "        # Reshape cho CrossEntropy: [Batch * SeqLen, NumTags]\n",
        "        loss = criterion(out.view(-1, NUM_TAGS), t.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def eval_ner(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for w, t in loader:\n",
        "            w, t = w.to(DEVICE), t.to(DEVICE)\n",
        "            out = model(w).argmax(dim=-1)\n",
        "            mask = (t != -100) # Chỉ tính các vị trí nhãn thực\n",
        "            correct += (out[mask] == t[mask]).sum().item()\n",
        "            total += mask.sum().item()\n",
        "    return correct / total\n",
        "\n",
        "print(\">>> Bắt đầu huấn luyện PhoNER...\")\n",
        "for epoch in range(10):\n",
        "    loss = train_ner(model, train_loader)\n",
        "    acc = eval_ner(model, dev_loader)\n",
        "    print(f\"Epoch {epoch+1:02} | Loss: {loss:.4f} | Dev Acc: {acc*100:.2f}%\")\n",
        "\n",
        "test_acc = eval_ner(model, test_loader)\n",
        "print(f\"\\nĐỘ CHÍNH XÁC TRÊN TẬP TEST: {test_acc*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huRww5-f3psL",
        "outputId": "e0fedf24-70a4-4e60-8072-d2cc870976e8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Bắt đầu huấn luyện PhoNER...\n",
            "Epoch 01 | Loss: 1.0790 | Dev Acc: 73.71%\n",
            "Epoch 02 | Loss: 0.7733 | Dev Acc: 78.13%\n",
            "Epoch 03 | Loss: 0.6264 | Dev Acc: 81.00%\n",
            "Epoch 04 | Loss: 0.5287 | Dev Acc: 82.89%\n",
            "Epoch 05 | Loss: 0.4727 | Dev Acc: 83.71%\n",
            "Epoch 06 | Loss: 0.4355 | Dev Acc: 84.67%\n",
            "Epoch 07 | Loss: 0.4069 | Dev Acc: 85.23%\n",
            "Epoch 08 | Loss: 0.3823 | Dev Acc: 85.94%\n",
            "Epoch 09 | Loss: 0.3654 | Dev Acc: 86.26%\n",
            "Epoch 10 | Loss: 0.3450 | Dev Acc: 86.72%\n",
            "\n",
            "ĐỘ CHÍNH XÁC TRÊN TẬP TEST: 85.98%\n"
          ]
        }
      ]
    }
  ]
}